Installing and Configuring the Cisco Opflex ML2 Configuration with SUSE OPENSTACK CLOUD 6 barclamp
Version 0.2
Author: Mike Friesenegger (mikef@suse.com)



This version of the document focuses on a reproducible set of steps to deploy and provide a basic configuration of SOC6 with a Cisco ACI fabric.  This document has achieved:
    - Cisco ACI ML2 Liberty driver implementation using SUSE built RPMs
        http://download.opensuse.org/repositories/Cloud:/OpenStack:/Liberty:/cisco-apic/SLE_12_SP1/
            # wget -r -N -np "http://download.opensuse.org/repositories/Cloud:/OpenStack:/Liberty:/cisco-apic/SLE_12_SP1/")
                into /srv/tftpboot/suse-12.1/x86_64/repos/PTF/rpm/ on admin node
            # createrepo-cloud-ptf on the admin node
    - Integration with SOC6
    - Deployment using SOC6 neutron barclamp
    - Creation of OpenStack system object in the Cisco APIC
    - Creation of the tenant and network in the Cisco APIC based on what was entered in the SOC6 horizon dashboard
    - Creation of a vxlan based network in Cisco APIC
    - Ability to start instances on different compute nodes that can ping their gateway and each other
    
Future versions of this document will address (in no particular order):
    - Communication of instances to external networks
    - "Bonding" of NICs connected to ACI fabric
    - HA of control plane and specifically Neutron server services

    

Prerequsites
    Start with a newly installed SOC6 with all nodes discovered and allocated.  (None of the openstack barclamps deployed yet)
        NOTE: This document assumes 192.168.124.0/24 for the admin network
        
1. Be sure the control node has an IP address that allows it to communicate to the Cisco APIC.
    RECOMMENDATION: Use external network and crowbar network allocate_ip to assign ip to control node

2. On compute nodes connected to the ACI fabric
    - Go into YAST | System | Network Settings
        Ignore chef client warning
    - Select eth device connected to the ACI fabric
    - Select "No Link and IP Setup"
    - In General tab | Set MTU = 1600
    - Save changes for that network device
    - Add a vlan device
    - Configuration name = vlan.4093
    - Select eth device for "Real Interface for VLAN"
    - Enter "4093" for the VLAN ID
    - Select "Dynamic Address - DHCP"
    - In General tab | Set MTU = 1600
    - Save changes for new VLAN device
    - Exit Network Settings
    - Go back into Network Settings
        Ignore chef client warning
    - Go into Routing tab
    - Add an entry to the Routing Table
        Destination     224.0.0.0
        Genmask         240.0.0.0
        Gateway         0.0.0.0
        Device          vlan.4093
    - Save and exit Network Settings
    - Exit YAST
    - ip a show dev vlan.4093
        - Verify that the interface has an IP address from the ACI fabric
        
3. Install openvswitch packages manually on control and compute nodes
    # vi /etc/modprobe.d/10-unsupported-modules.conf
	change allow_unsupported_modules to 1
    # zypper in openvswitch openvswitch-switch openvswitch-kmp-default
    # modinfo openvswitch
        Must be 2.4.1
    
4. Create required ovs bridges
    - on control node:
        # systemctl start openvswitch
        # systemctl enable openvswitch
        # ovs-vsctl add-br br-int
    - on compute nodes:
        # systemctl start openvswitch
        # systemctl enable openvswitch
        # ovs-vsctl add-br br-int
        # ovs-vsctl set bridge br-int protocols=[]
        # ovs-vsctl add-port br-int br-int_vxlan0 -- set Interface br-int_vxlan0 type=vxlan options:remote_ip=flow options:key=flow options:dst_port=8472
        # lsmod | grep openvswitch
            Verify a line of the output shows
                openvswitch           130813  1 vport_vxlan

                
5. On compute nodes
    # zypper in lldpd
    # systemctl start lldpd.service
    # lldpctl
        - Be patient because it take a time to scan LLDP neighbors
    - Make a note of PortDescr: line for the eth interface(s) on the ACI fabric. An example is:
        topology/pod-1/paths-101/pathep-[eth1/33]

6. Deploy all pertainent OpenStack barclamps using Crowbar web interface.  For the Neutron barclamp:
    Create a proposal and click on "raw" to edit the attributes in raw mode. Or on the command line:
        # crowbar neutron proposal create default
        # crowbar neutron proposal edit default

    Set the following values (leave everything else set to the defaults):
{
  "create_default_networks": false,
  "dhcp_domain": "cloud.cisco-aci.lab",
  "use_lbaas": false,
  "ml2_mechanism_drivers": [
    "cisco_apic_ml2"
  ],
  "ml2_type_drivers": [
    "opflex",
    "vxlan"
  ],
  "ml2_type_drivers_default_provider_network": "opflex",
  "ml2_type_drivers_default_tenant_network": "opflex",
  "apic": {
    "hosts": "10.105.1.10",
    "system_id": "soc6",
    "username": "admin",
    "password": "cisco123"
  },
  "apic_switches": {
    "101": {
      "switch_ports": {
        "d58-ac-78-f2-61-6d": {
          "switch_port": "1/2"
        },
        "dcc-46-d6-c0-6a-87": {
          "switch_port": "1/33"
        }
      }
    }
  }
}

        - Edit the hosts, system_id, username, password and apic_switches to match ACI fabric
            For the apic_switches section:
                - Update 101 and create any new sections like 101 to match number(s) in paths-XXX
                    Example: paths-101 and paths-102 would result in keeping the 101 section and adding a new 102 section
                - Update mac address entry with output for each host and the switch port it is connected to
                    Example: get hostname of compute node by using "hostname" command and [eth1/33] from example above would result in
                        "dcc-46-d6-c0-6a-87": {
                          "switch_port": "1/33"
                        },
                    Multiple <hostname> and switch_port combination can exist under apic_switches section
        - Save the changes
        - Edit the deployment if the automatic selection somehow didn't work (drag and drop nodes to the different roles as required)
        - Apply either by clicking "Apply" in the WebUI or by running:
            # crowbar neutron proposal commit default
        - Return to the Crowbar web interface and apply the neutron barclamp and remaining barclamps
            IF NOVA BARCLAMP FAILS SEE TROUBLESHOOTING SECTION
                
7. In horizon dashboard
    - On admin node
        # scp SLES_12_SP1_JeOS_OpenStack-no-cloud-init.x86_64-0.0.4.qcow2 soc-control:/root
    - On control node
        # source .openrc
        # glance image-create --name sles-jeos --file SLES_12_SP1_JeOS_OpenStack-no-cloud-init.x86_64-0.0.4.qcow2 --disk-format qcow2 --container-format=bare
        - Make image public in web interface
    - Create a new network (Project | Network | Networks)
    - Verify that the network is created in the APIC web interface under Tenants
    - Start instances on each compute node within the same project that are attached to the new network
    - Confirm the instances and ping their gateway and each other
    
    

TROUBLESHOOTING FROM THIS ATTEMPT
    - Applying the nova barclamp will fail on the first attempt to apply barclamp
Failed to apply the proposal to: dcc-46-d6-c0-6a-87.cloud.cisco-aci.lab 
[2016-06-21T14:48:53-07:00] ERROR: Running exception handlers 
[2016-06-21T14:48:53-07:00] FATAL: Saving node information to /var/chef/cache/failed-run-data.json 
[2016-06-21T14:48:53-07:00] ERROR: Exception handlers complete 
[2016-06-21T14:48:53-07:00] FATAL: Stacktrace dumped to /var/chef/cache/chef-stacktrace.out 
[2016-06-21T14:48:53-07:00] FATAL: Chef::Exceptions::Exec: service[nova-compute] (nova::compute line 23) had an error: Chef::Exceptions::Exec: /bin/systemctl start openstack-nova-compute returned 6, expected 0
        systemctl lists no available openstack services
        on all compute nodes
            # systemctl daemon-reload
        Reapply and the barclamp will complete successfully
    - If the instances are unable to ping the gateway or another instance on a different compute node at all or pings are unreliable
        - Terminate instances
        - Delete network
        - Reboot compute nodes
        - Create new network
        - Start instances
        - They will be able to ping their gateway and other instance
        
